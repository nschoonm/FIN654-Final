---
title: 'Final'
#author:
#- Nicolas Schoonmaker
#- Guillermo Delgado
#- Katie Guillen
#- Leanne Harper
#date: "`r format(Sys.time(), '%m/%d/%Y')`"
output: 
  flexdashboard::flex_dashboard:
    orientation: columns
    theme: "spacelab"
runtime: shiny
---

Background
=======================================================================

Column
-----------------------------------------------------------------------

### Purpose, Process, Product

We will represent a wholesale distributor. We will compute optimal holdings of risky and risk-free assets for the Markowitz mean-variance model. We will then build a simple financial web application. With this tool we can also explore impact of the extremes of distributions of financial returns on portfolio results. 

### Problem

We are a wholesale distributor of soybean and wheat; we are considering adding corn to our portfolio of commodities.  We currently distribute our product to food processors, animal feed producers, exporters, and other buyers.  We feel that many of our current customers will be attracted to this added commodity as it is used in many of their end products.  We will use future contracts as a hedging tool (leverage) to help mitigate risk.  Future contracts will allow the company flexibility in managing price movement speculation.  The future contracts will allow us to take long or short positions depending on future market expectations.  They have allocated $250 million to purchase commodoties. The company wants us to:

1.	Retrieve and begin to analyze data about potential commodities to diversify into
2.	Compare potential commodities with existing commodities in conventional agricultural spot markets
3.	Begin to generate economic scenarios based on events that may, or may not, materialize in the commodities
4.	The company wants to mitigate their risk by using future contracts, which will require closely monotoring the agricultural spot market and continued speculation on future price movements.

### Data and analysis to inform the decision

Data analysis highlights:

- Spot market prices of Corn, Soybean, and Wheat
- Corn and Soybean: correlation
- Corn and Wheat: correlation
- Soybean and Wheat: Correlation
- Corn and Soybean: Correlation sensitivity to Soybean dependency
- All together: correlations and volaitlities among these indicators
- Cross-section of rolling correlation will be visualize correlation
- Value at Risk (VaR) represents our maximum expected loss given a certain confidence level and Expected Shortfall represents the expected loss in extreme cases, when the loss exceeds VaR


Column
-----------------------------------------------------------------------

### Method

Identify the optimal combination of Corn, Soybean, and Wheat to trade

1.	Product: Agricultural commodities and future contracts
2.	Commodity, Company, and Geography:
    a. Corn: West Bend, Iowa, United States
    b. Soybean: Mato Grosso, Brazil
    c. Wheat: Beijing, China
3.	Customers: food processors, animal feed producers, exporters, and other buyers
4.  All commodities traded on the Chicago Board of Trade commodity exchanges

### Stylized facts of the Agriculture market

The Chicago Board of Trade (CBOT) is one of the major and most known commodity exchanges operating with agricultural commodities such as corn, soybeans, wheat, oats, and rice.  Today, CBOT is part of the Chicago Mercantile Exchange (CME) Group.

<TODO - Update based on Chicago board of trade>:

- Volatility is rarely constant and often has a structure (mean reversion) and is dependent on the past.  In other words, over time the returns will move back to its average historical levels.  For the most part, it typically rises after it it falls too low and falls after rising too high.
- Extreme events are likely to happen with other extreme events.  For instance, extreme weather events or an abundance of crop planting may affect supply and demand and drive price to extremes.
- Negative returns are more likely than positive returns (left skew) but tends to have light tails, with very few outliers (skewness level close to 3) for wheat.  For both corn and wheat, we see the opposite.  There is a higher likelihood of positive returns with more extreme outliers.

### Key business questions

The following are the key business questions that will be answered in the conclusion section.

1.	How would we manage the allocation of existing resources given we have just landed in this new market?
2.	How does this decision impact the business?
3.	What are your recommendations? 


Approach
-----------------------------------------------------------------------


### History speaks

- We will develop the *value at risk* and *expected shortfall* metrics from the historical simulated distributions of risk factors.
- Given these factors we will combine them into a portfolio and calculate their losses. 
- With the loss distribution in hand we can compute the risk measures. - This approach is nonparametric.

- We can then posit high quantile thresholds and explore risk measures the in the tails of the distributions.

First we set the tolerance level $\Large\alpha$, for example, equal to 95\%. This would mean that a decision maker would not tolerate loss in  more than $\Large 1-\alpha$, or 5\%. of all risk scenarios under consideration.

We define the VaR as the quantile for probability $\Large\alpha \in (0,1)$, as

$$
\Large
VaR_{\alpha} (X) = inf \{ x \in R: F(x) \geq \alpha \},
$$

which means find the greatest lower bound of loss $\Large x$ (what the symbol $\Large inf$ = _infimum_ means in English), such that the cumulative probability of $\Large x$ is greater than or equal to $\Large \alpha$. 

Using the $\Large VaR_{\alpha}$ definition we can also define $\Large ES$ as

$$
\Large
ES_{\alpha} = E [X \lvert X \geq VaR_{\alpha}],
$$

where $\Large ES$ is "expected shortfall" and $\Large E$ is the expectation operator, also known as the "mean." Again, in English, the expected shortfall is the average of all losses greater than the loss at a $\Large VaR$ associated with probability $\Large \alpha$, and $\Large ES \geq VaR$.


### Getting to a reponse: more detailed questions

1. What is the decision the wholesale distributor must make? List key business questions and data needed to help answer these questions and support the wholesale distrubotor's decision.

2. Develop a model to optimize the holdings of each of the three commodities; corn, soybean, wheat 

3. Run two scenarios: with and without short sales of the commodities. 

4. Interpret results for the wholesale distributor, including tangency portfolio, amount of cash and equivalents in the portfolio allocation, minimum risk portfolio and the risk and return characteristics of each commodity.


```{r Get current directory}
# Get the directory so we can run this from anywhere
# Get the script directory from R when running in R
if(rstudioapi::isAvailable())
{
  script.path <- rstudioapi::getActiveDocumentContext()$path
  script.dir <- dirname(script.path)
}
if(!exists("script.dir"))
{
  script.dir <- getSrcDirectory(function(x) {x})
}
```

```{r Working Directory and Data setup}
# Set my working directory
# There is a "data" folder here with the files and the script
setwd(script.dir)
# Double check the working directory
getwd()
# Error check to ensure the working directory is set up and the data
# directory exists inside it.  Its required for this file
if(dir.exists(paste(getwd(),"/data", sep = "")) == FALSE) {
  stop("Data directory does not exist. Make sure the working directory
       is set using setwd() and the data folder exists in it.")
}
```

```{r Setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)

library(ggplot2)
library(flexdashboard)
library(shiny)
library(QRM)
library(qrmdata)
library(xts)
library(zoo)
library(plotly)
#library(ggfortify)
library(psych)

rm(list = ls())
# PAGE: Exploratory Analysis
data <- na.omit(read.csv("data/ingredients.csv", header = TRUE))
#data$DATE <- as.Date(data$DATE, "%m/%d/%Y")
#data <- data[order(data$DATE),]
# Compute log differences percent using as.matrix to force numeric type
data.r <- diff(log(as.matrix(data[, -1]))) * 100
# Create size and direction
size <- na.omit(abs(data.r)) # size is indicator of volatility
#head(size)
colnames(size) <- paste(colnames(size),".size", sep = "") # Teetor
direction <- ifelse(data.r > 0, 1, ifelse(data.r < 0, -1, 0)) # another indicator of volatility
colnames(direction) <- paste(colnames(direction),".dir", sep = "")
# Convert into a time series object: 
# 1. Split into date and rates
dates <- as.Date(data$DATE[-1], "%m/%d/%Y")
dates.chr <- as.character(data$DATE[-1])
str(dates.chr)
values <- cbind(data.r, size, direction)
# for dplyr pivoting and ggplot2 need a data frame also known as "tidy data"
data.df <- data.frame(dates = dates, returns = data.r, size = size, direction = direction)
data.df.nd <- data.frame(dates = dates.chr, returns = data.r, size = size, direction = direction, stringsAsFactors = FALSE) 
#non-coerced dates for subsetting on non-date columns
# 2. Make an xts object with row names equal to the dates
data.xts <- na.omit(as.xts(values, dates)) #order.by=as.Date(dates, "%d/%m/%Y")))
#str(data.xts)
data.zr <- as.zooreg(data.xts)
returns <- data.xts

# PAGE: Market risk 
corr.rolling <- function(x) {	
  dim <- ncol(x)	
  corr.r <- cor(x)[lower.tri(diag(dim), diag = FALSE)]	
  return(corr.r)	
}

ALL.r <- data.xts[, 1:3]
window <- 90 #reactive({input$window})
corr.returns <- rollapply(ALL.r, width = window, corr.rolling, align = "right", by.column = FALSE)
corr.returns.df <- data.frame(Date = index(corr.returns), Corn.Soybean = corr.returns[,1], Corn.Wheat = corr.returns[,2], Soybean.Wheat = corr.returns[,3])

# Market dependencies
library(matrixStats)
R.corr <- apply.monthly(as.xts(ALL.r), FUN = cor)
R.vols <- apply.monthly(ALL.r, FUN = colSds) # from MatrixStats	
# Form correlation matrix for one month 	
R.corr.1 <- matrix(R.corr[20,], nrow = 3, ncol = 3, byrow = FALSE)	
rownames(R.corr.1) <- colnames(ALL.r[,1:3])	
colnames(R.corr.1) <- rownames(R.corr.1)	
R.corr <- R.corr[, c(2, 3, 6)]
colnames(R.corr) <- c("Corn & Soybean", "Corn & Wheat", "Soybean & Wheat")
colnames(R.vols) <- c("Corn.vols", "Soybean.vols", "Wheat.vols")	
R.corr.vols <- na.omit(merge(R.corr, R.vols))
Corn.vols <- as.numeric(R.corr.vols[,"Corn.vols"])	
Soybean.vols <- as.numeric(R.corr.vols[,"Soybean.vols"])	
Wheat.vols <- as.numeric(R.corr.vols[,"Wheat.vols"])
library(quantreg)
# hist(rho.fisher[, 1])
Corn.corrs <- R.corr.vols[,1]
#hist(Corn.corrs)
taus <- seq(.05,.95,.05)	# Roger Koenker UI Bob Hogg and Allen Craig
fit.rq.Corn.Soybean <- rq(Corn.corrs ~ Soybean.vols, tau = taus)	
fit.lm.Corn.Soybean <- lm(Corn.corrs ~ Soybean.vols)	
#' Some test statements	
#summary(fit.rq.Corn.Soybean, se = "boot")
#'
#summary(fit.lm.Corn.Soybean, se = "boot")
#plot(summary(fit.rq.Corn.Soybean), parm = "Soybean.vols", main = "Corn-Soybean correlation sensitivity to Soybean volatility") #, ylim = c(-0.1 , 0.1))	
```


Data
=======================================================================

Column
-----------------------------------------------------------------------

### Data Definitions

- *Corn*: daily Corn price (\$/per bushel)
- *Soybean*: daily Soybean prices (\$/per bushel)
- *Wheat *: daily Wheat prices (\$/per bushel)


### Historical data 2015-2020

Notes about the historical data for Corn, Soybean and Wheat.

- <TODO> Describe historical data
- Corn has experienced a number of spikes in price and magnitude percentage change. In our data set corn experienced the lowest return at the end of June 2015 and the highest return in August 2019. 
- Soybean is less volatile in terms of price and magnitude percentage change.
- Wheat experienced the most volatility clustering of all the grains.  We see that small changes are followed by small changes and large changes tend to be followed by large changes.

```{r Data summary}
summary(data.r)
```

### Data and markets

The following lists data sources and markets for trading. 

- https://www.cmegroup.com/trading/agricultural/ for daily trading data
- https://www.macrotrends.net/2534/wheat-prices-historical-chart-data for historical wheat data
- https://www.macrotrends.net/2532/corn-prices-historical-chart-data for historical corn data
- https://www.macrotrends.net/2531/soybean-prices-historical-chart-data for historical soybean data
- Github and Stack Overflow for trouble shooting

Column
-----------------------------------------------------------------------

### Commodity Price Percent Changes
```{r Price Changes}
renderPlotly({
  library(ggplot2)
  #library(ggfortify)
  library(plotly)
  #title.chg1 <- "Ingrediant Price Percent Changes"
  #title.chg2 <- "Size of Ingrediants Price Percent Changes"
  p <- autoplot.zoo(data.xts[,1:3]) # + ggtitle(title.chg1) #+ ylim(-5, 5)
  ggplotly(p)
})
```

### Size of Commodity Price Percent Changes

```{r Size of Price Changes}
renderPlotly({
  #title.chg1 <- "Ingrediants Price Percent Changes"
  #title.chg2 <- "Size of Ingrediants Price Percent Changes"
  p <- autoplot.zoo(abs(data.xts[,1:3])) # + ggtitle(title.chg2) #+ ylim(-5, 5)
  ggplotly(p)
  })
```



Exploratory
=======================================================================

Inputs {.sidebar}
-----------------------------------------------------------------------
A quantile divides the returns distribution into two groups. For example 75\% of all returns may fall below a return value of 10\%. The distribution is thus divided into returns above 10\% and below 10\% at the 75\% quantile.

Pull slide to the right to measure the risk of returns at desired quantile levels. The minimum risk quantile is 75\%. The maximum risk quantile is 99\%.


```{r Exploratory}
sliderInput("alphaq", label = "Risk Measure quantiles (%):",
            min = 0.75, max = 0.99, value = 0.75, step = 0.01)
```

Columnn {data-width=200}
-----------------------------------------------------------------------

### Corn Value at Risk

```{r Corn Value at Risk}
#threshold <- reactive({input$threshold.q}) #BE SURE that {} included 
renderValueBox({
  new_alpha1 <- input$alphaq
  alpha1 <- ifelse(new_alpha1>1,0.99,ifelse(new_alpha1<0,0.001,new_alpha1))
  returns1 <- returns[,1]
  colnames(returns1) <- "Returns" #kluge to coerce column name for df below
  q <- quantile(returns1,alpha1)
  VaR.hist <- q
  valueBox(round(VaR.hist, 2),
           icon = "fa-ship", color = "light-blue")
})
```

### Soybean Value at Risk

```{r Soybean Value at Risk}
#threshold <- reactive({input$threshold.q}) #BE SURE that {} included 
renderValueBox({
  new_alpha1 <- input$alphaq
  alpha1 <- ifelse(new_alpha1>1,0.99,ifelse(new_alpha1<0,0.001,new_alpha1))
  returns1 <- returns[,2]
  colnames(returns1) <- "Returns" #kluge to coerce column name for df below
  q <- quantile(returns1,alpha1)
  VaR.hist <- q
  valueBox(round(VaR.hist, 2),
           icon = "fa-ship", color = "light-blue")
})
```

### Wheat Value at Risk

```{r What Value at Risk}
#threshold <- reactive({input$threshold.q}) #BE SURE that {} included 
renderValueBox({
  new_alpha1 <- input$alphaq
  alpha1 <- ifelse(new_alpha1>1,0.99,ifelse(new_alpha1<0,0.001,new_alpha1))
  returns1 <- returns[,3]
  colnames(returns1) <- "Returns" #kluge to coerce column name for df below
  q <- quantile(returns1,alpha1)
  VaR.hist <- q
  valueBox(round(VaR.hist, 2),
           icon = "fa-ship", color = "light-blue")
})
```

Column {.tabset .tabset-fade}
-----------------------------------------------------------------------

### Corn Returns Distribution

```{r Corn Returns}
renderPlotly({
  returns1 <- returns[,1]
  colnames(returns1) <- "Returns" #kluge to coerce column name for df
  returns1.df <- data.frame(Returns = returns1[,1], 
                            Distribution = rep("Historical", each = length(returns1)))
  
  new_alpha1 <- input$alphaq
  alpha1 <- ifelse(new_alpha1>1,0.99,ifelse(new_alpha1<0,0.001,new_alpha1))
  
  # Value at Risk
  VaR1.hist <- quantile(returns1,alpha1)
  VaR1.text <- paste("Value at Risk =", round(VaR1.hist, 2))
  
  # Determine the max y value of the desity plot.
  # This will be used to place the text above the plot
  VaR1.y <- max(density(returns1.df$Returns)$y)
  
  # Expected Shortfall
  ES1.hist <- median(returns1[returns1 > VaR1.hist])
  ES1.text <- paste("Expected Shortfall =", round(ES1.hist, 2))
  
  p1 <- ggplot(returns1.df, aes(x = Returns, fill = Distribution)) + 
    geom_density(alpha = 0.5) + 
    geom_vline(aes(xintercept = VaR1.hist), linetype = "dashed", size = 1, color = "firebrick1") + 
    geom_vline(aes(xintercept = ES1.hist), size = 1, color = "firebrick1") +
    annotate("text", x = 1+ VaR1.hist, y = VaR1.y*1.05, label = VaR1.text) +
    annotate("text", x = 0.5+ ES1.hist, y = VaR1.y*1.1, label = ES1.text) +
    scale_fill_manual(values = "dodgerblue4")
  p1	##ggplotly(p)
  })
```

###  Soybean Returns Distribution

```{r Soybean Returns}
renderPlotly({
  returns2 <- returns[,2]
  colnames(returns2) <- "Returns" #kluge to coerce column name for df
  returns2.df <- data.frame(Returns = returns2[,1], 
                            Distribution = rep("Historical", each = length(returns2)))
  
  alpha2 <- ifelse(input$alphaq>1,0.99,ifelse(input$alphaq<0,0.001,input$alphaq))
  
  # Value at Risk
  VaR2.hist <- quantile(returns2,alpha2)
  VaR2.text <- paste("Value at Risk =", round(VaR2.hist, 2))
  
  # Determine the max y value of the desity plot.
  # This will be used to place the text above the plot
  VaR2.y <- max(density(returns2.df$Returns)$y)
  
  # Expected Shortfall
  ES2.hist <- median(returns2[returns2 > VaR2.hist])
  ES2.text <- paste("Expected Shortfall =", round(ES2.hist, 2))
  
  p2 <- ggplot(returns2.df, aes(x = Returns, fill = Distribution)) + 
    geom_density(alpha = 0.5) + 
    geom_vline(aes(xintercept = VaR2.hist), linetype = "dashed", size = 1, color = "firebrick1") + 
    geom_vline(aes(xintercept = ES2.hist), size = 1, color = "firebrick1") +
    annotate("text", x = 1+VaR2.hist, y = VaR2.y*1.05, label = VaR2.text) +
    annotate("text", x = 0.5+ES2.hist, y = VaR2.y*1.1, label = ES2.text) + 
    scale_fill_manual(values = "dodgerblue4")
  p2	##ggplotly(p)
})
```

### Wheat Returns Distribution

```{r Wheat Returns}
renderPlotly({
  returns3 <- returns[,3]
  colnames(returns3) <- "Returns" #kluge to coerce column name for df
  returns3.df <- data.frame(Returns = returns3[,1], 
                            Distribution = rep("Historical", each = length(returns3)))
  ggplot(returns3.df, aes(x = Returns, fill = Distribution)) + geom_density(alpha = 0.8)
  
  alpha3 <- ifelse(input$alphaq>1,0.99,ifelse(input$alphaq<0,0.001,input$alphaq))
  
  # Value at Risk
  VaR3.hist <- quantile(returns3,alpha3)
  VaR3.text <- paste("Value at Risk =", round(VaR3.hist, 2))
  
  # Determine the max y value of the desity plot.
  # This will be used to place the text above the plot
  VaR3.y <- max(density(returns3.df$Returns)$y)
  
  # Expected Shortfall
  ES3.hist <- median(returns3[returns3 > VaR3.hist])
  ES3.text <- paste("Expected Shortfall =", round(ES3.hist, 2))
  
  p3 <- ggplot(returns3.df, aes(x = Returns, fill = Distribution)) + 
    geom_density(alpha = 0.5) + 
    geom_vline(aes(xintercept = VaR3.hist), linetype = "dashed", size = 1, color = "firebrick1") + 
    geom_vline(aes(xintercept = ES3.hist), size = 1, color = "firebrick1") +
    annotate("text", x = 1+VaR3.hist, y = VaR3.y*1.05, label = VaR3.text) +
    annotate("text", x = 0.5+ES3.hist, y = VaR3.y*1.1, label = ES3.text) +
    scale_fill_manual(values = "dodgerblue4")
  p3	##ggplotly(p)
})

```

### ACF

```{r ACF}
require(graphics)
renderPlot({
  returnz.value <- acf(coredata(data.xts[,1:3])) # returns
  plot(returnz.value)
})
```

```{r Correlation}
require(graphics)
renderPlot({
  returnz.size <- acf(coredata(data.xts[,4:5])) # sizes
  plot(returnz.size)
})
```

### Statistics

```{r Statistics}
## data_moments function
## INPUTS: r vector
## OUTPUTS: list of scalars (mean, sd, median, skewness, kurtosis)
data_moments <- function(data){
  library(moments)
  library(matrixStats)
  mean.r <- colMeans(data)
  median.r <- colMedians(data)
  sd.r <- colSds(data)
  IQR.r <- colIQRs(data)
  skewness.r <- skewness(data)
  kurtosis.r <- kurtosis(data)
  result <- data.frame(mean = mean.r, 
                       median = median.r, 
                       std_dev = sd.r, 
                       IQR = IQR.r, 
                       skewness = skewness.r, 
                       kurtosis = kurtosis.r)
  return(result)
}
# Run data_moments()
answer <- data_moments(data.xts[, 1:3])
# Build pretty table
answer <- round(answer, 4)
knitr::kable(answer)
```


Market Risk
=======================================================================

Column
-----------------------------------------------------------------------

### Corn, Soybean, Wheat Observations
Relationship and correlation observations.

- <TODO> Update all this, its still based off metal information
- The scatterplot illustrates some of the outliers of each of the three ingrediants.  Wheat appears to be more concentrated around the mean but also experienced its share of volatility
- Soybean is also closely grouped around its mean
- Corn experienced the most volatility with points appearing the furthest from the mean.
- There is a strong correlation (.89) between Corn and Soybean.  This was initially expected since both of these ingrediant are used in similar applications or in the creation of bread? which is used for a significant number of base foods.  
- Corn is also used in a variety of other applications ranging from minting, armaments, marine engineering, electrical applications, and for the repair of fan blades found in geothermal power plants.  


### Corn, Soybean, Wheat relationships

```{r Relationships}
#library(psych)
pairs.panels(corr.returns.df)
```

### Getting practical
Useful information about market risk.

- Using price returns we can compute loss. 

- Weights for each are defined as the value of the positions in each risk factor. 

- We can compute this as the notional (in tonnes equivalent for this market) times the last observed price.

- Losses for a barrel equivalent of the three commodities are computed back into the sample relative to the most recently observed prices


Column {.tabset }
-----------------------------------------------------------------------

### Corn and Soybean (90 day rolling correlation)

```{r Corn and Soybean Rolling Correlation}
renderPlotly({
  p <- ggplot(corr.returns.df, aes(x = Date, y = Corn.Soybean)) + geom_line()
  p	##ggplotly(p)
})
```

### Corn and Wheat (90 day rolling correlation)

```{r Corn and Wheat Rolling Correlation}
renderPlotly({
  p <- ggplot(corr.returns.df, aes(x = Date, y = Corn.Wheat)) + geom_line()
  p	##ggplotly(p)
})
```

### Soybean and Wheat (90 day rolling correlation)

```{r Soybean and Wheat Rolling Correlation}
renderPlotly({
  p <- ggplot(corr.returns.df, aes(x = Date, y = Soybean.Wheat)) + geom_line()
  p	##ggplotly(p)
})
```

### 30 day within-sample correlations and volatilities

```{r Correlations and Vols}
plot.zoo(R.corr.vols, main= "Monthly Correlations and Volatilities")
```

### Corn - Soybean Dependency

```{r Corn and Soybean Dependency}
renderPlot({ 
  plot(summary(fit.rq.Corn.Soybean), 
       parm = "Soybean.vols", 
       main = "Corn-Soybean correlation sensitivity to Soybean volatility")
})
```

- Assume that the loss density $\Large f_L$ is strictly positive so that the distribution function of loss possesses a diffentiable inverse and change variables so that $\Large v = q_u(L) = F_L(u)$ the cumulative loss distribution. Then 

$$
\Large
\frac{dv}{du} = f^{-1}(v)
$$
and we can compute
$$
\Large
\frac{\partial r_{ES}^{\alpha}}{\partial \lambda_i}(1) = \frac{1}{1-\alpha}\int_{q_{\alpha}(L)}^{\infty}E(L_i | L=v)f_L(v)dv = \frac{1}{1-\alpha}\int_{\alpha}^1E(L_i \, | \, L \geq q_{\alpha}(L))
$$
- (Finally) we have the expected shortfall contribution of a line of business $\Large i$ as
$$
\Large
C_i^{ES} = E(L_i | L \geq VaR_{\alpha}(L))
$$



### Empirical loss

```{r Tolerance Input}
sliderInput("losstol", label = "Loss Tolerance (%):",
            min = 0.50, max = 0.99, value = 0.95, step = 0.01)
```

```{r Loss Analysis}
## Now for Loss Analysis
# Get last prices

price.last <- as.numeric(tail(data[, -1], n=1))
# Specify the positions
position.rf <- c(1/3, 1/3, 1/3)
# And compute the position weights
w <- position.rf * price.last
# Fan these  the length and breadth of the risk factor series
weights.rf <- matrix(w, nrow=nrow(data.r), ncol=ncol(data.r), byrow=TRUE)
#head(rowSums((exp(data.r/100)-1)*weights.rf), n=3)
## We need to compute exp(x) - 1 for very small x: expm1 accomplishes this
#head(rowSums((exp(data.r/100)-1)*weights.rf), n=4)
loss.rf <- -rowSums(expm1(data.r/100) * weights.rf)
loss.rf.df <- data.frame(Loss = loss.rf, Distribution = rep("Historical", each = length(loss.rf)))
## Simple Value at Risk and Expected Shortfall
renderPlotly({
  alpha.tolerance <- input$losstol
  #alpha.tolerance <- .95
  VaR.hist <- quantile(loss.rf, probs=alpha.tolerance, names=FALSE)
  ## Just as simple Expected shortfall
  ES.hist <- median(loss.rf[loss.rf > VaR.hist])
  VaR.text <- paste("Value at Risk =\n", round(VaR.hist, 4)) # ="VaR"&c12
  ES.text <- paste("Expected Shortfall \n=", round(ES.hist, 4))
  title.text <- paste(round(alpha.tolerance*100, 0), "% Loss Limits")
  p <- ggplot(loss.rf.df, aes(x = Loss, fill = Distribution)) +
    geom_histogram(alpha = 0.8, bins=30) +
    geom_vline(aes(xintercept = VaR.hist), linetype = "dashed", size = 1, color = "blue") +
    geom_vline(aes(xintercept = ES.hist), size = 1, color = "blue") +
    annotate("text", x = VaR.hist, y = 200, label = VaR.text) +
    annotate("text", x = ES.hist, y = 100, label = ES.text) +
    xlim(-0.5, 0.5) + ggtitle(title.text)
  p	##ggplotly(p)
})
```

Extremes
=======================================================================
Column {.tabset}
-----------------------------------------------------------------------
### Let's go to extremes

- All along we have been stylizing financial returns, commodities and exchange rates, as skewed and with thick tails.
- We next go on to an extreme tail distribution called the Generalized Pareto Distribution (GPD). 
- For very high thresholds, GPD not only well describes behavior in excess of the threshold, but the mean excess over the threshold is linear in the threshold. 
- From this we get more intuition around the use of expected shortfall as a coherent risk measure. 
- In recent years we well exceeded all Gaussian and Student's t thresholds.

For a random variate $\Large x$, this distribution is defined for the shape parameters $\Large \xi \geq 0$ as:

$$
\Large
g(x; \xi \geq 0) = 1- (1 + x \xi/\beta)^{-1/\xi}
$$


and when the shape parameter $\Large \xi = 0$, the GPD becomes the exponential distribution dependent only on the scale parameter $\beta$:

$$
\Large
g(x; \xi = 0) = 1 - exp(-x/\beta).
$$

Now for one reason for GPD's notoriety...

- If $\Large u$ is an upper (very high) threshold, then the excess of threshold function for the GPD is

$$
\Large
e(u) = \frac{\beta + \xi u}{1 - \xi}.
$$

- This simple measure is _linear_ in thresholds. 
- It will allow us to visualize where rare events begin (see McNeil, Embrechts, and Frei (2015, chapter 5)). 
- we often exploit this property when we look at operational loss data.
- Here is a mean excess loss plot for the `loss.rf` data. If there is a straight-line relationship after a threshold, then we have some evidence for the existence of a GPD for the tail.

### Mean excess loss

```{r Extreme}
# mean excess plot to determine thresholds for extreme event management
data <- as.vector(loss.rf) # data is purely numeric
umin <-  min(data)         # threshold u min
umax <-  max(data) - 0.1   # threshold u max
nint <- 100                # grid length to generate mean excess plot
grid.0 <- numeric(nint)    # grid store
e <- grid.0                # store mean exceedances e
upper <- grid.0            # store upper confidence interval
lower <- grid.0            # store lower confidence interval
u <- seq(umin, umax, length = nint) # threshold u grid
alpha <- 0.95                  # confidence level
for (i in 1:nint) {
  data <- data[data > u[i]]  # subset data above thresholds
  e[i] <- mean(data - u[i])  # calculate mean excess of threshold
  sdev <- sqrt(var(data))    # standard deviation
  n <- length(data)          # sample size of subsetted data above thresholds
  upper[i] <- e[i] + (qnorm((1 + alpha)/2) * sdev)/sqrt(n) # upper confidence interval
  lower[i] <- e[i] - (qnorm((1 + alpha)/2) * sdev)/sqrt(n) # lower confidence interval
}
mep.df <- data.frame(threshold = u, threshold.exceedances = e, lower = lower, upper = upper)
loss.excess <- loss.rf[loss.rf > u]
# Voila the plot => you may need to tweak these limits!
renderPlotly({
  p <- ggplot(mep.df, aes( x= threshold, y = threshold.exceedances)) +
    geom_line() +
    geom_line(aes(x = threshold, y = lower), colour = "red") +
    geom_line(aes(x = threshold, y = upper), colour = "red") +
    annotate("text", x = 0.2, y = 0.2, label = "upper 95%") +
    annotate("text", x = 0.2, y = -0.1, label = "lower 5%")
  p ##ggplotly(p)
})
```

### GPD fits and starts

```{r GPD}
#library(QRM)
alpha.tolerance <- 0.95
u <- quantile(loss.rf, alpha.tolerance , names=FALSE)
fit <- fit.GPD(loss.rf, threshold=u) # Fit GPD to the excesses
xi.hat <- fit$par.ests[["xi"]] # fitted xi
beta.hat <- fit$par.ests[["beta"]] # fitted beta
data <- loss.rf
n.relative.excess <- length(loss.excess) / length(loss.rf) # = N_u/n
VaR.gpd <- u + (beta.hat/xi.hat)*(((1-alpha.tolerance) / n.relative.excess)^(-xi.hat)-1) 
ES.gpd <- (VaR.gpd + beta.hat-xi.hat*u) / (1-xi.hat)
n.relative.excess <- length(loss.excess) / length(loss.rf) # = N_u/n
VaR.gpd <- u + (beta.hat/xi.hat)*(((1-alpha.tolerance) / n.relative.excess)^(-xi.hat)-1) 
ES.gpd <- (VaR.gpd + beta.hat-xi.hat*u) / (1-xi.hat)
# Plot away
renderPlotly({
  VaRgpd.text <- paste("GPD: Value at Risk =", round(VaR.gpd, 2))
  ESgpd.text <- paste("Expected Shortfall =", round(ES.gpd, 2))
  title.text <- paste(VaRgpd.text, ESgpd.text, sep = " ")
  loss.plot <- ggplot(loss.rf.df, aes(x = Loss, fill = Distribution)) +
    geom_density(alpha = 0.2)
  loss.plot <- loss.plot + 
    geom_vline(aes(xintercept = VaR.gpd), colour = "blue", linetype = "dashed", size = 0.8)
  loss.plot <- loss.plot + 
    geom_vline(aes(xintercept = ES.gpd), colour = "blue", size = 0.8) 
  loss.plot <- loss.plot + xlim(0,0.3) + ggtitle(title.text)
  loss.plot	##ggplotly(loss.plot)
})
```

### Confidence and risk measures

```{r BFGS}
showRM(fit, alpha = 0.99, RM = "ES", method = "BFGS")
```


Optimization
==========================================================
Column {.tabset}
----------------------------------------------------------

### If that wasn't enough...

Stylized market facts indicate

- Allocation across various component of loss drivers requires both body and tail considerations
- Pessisimistic risk measurement requires some sort of distortion measure to assess the probability of good and bad news

So that ...

- Bassett et al.(2004) show that the mean-expected shortfall efficient portfolio problem is equivalent to a quantile regression with linear constraints.
- Enlarge scope of expected utility from monetary and probabality to include an assessment (distortion) of probability.
- Choquet integrals build on Lebesgue measures by inflating or deflating the probabilities by the rank order of the outcomes.
- Expected shortfall is an example of a Choquet, rank-ordered, criterion.

**The risk-free rate was**

- obtained from https://www.treasury.gov/resource-center/data-chart-center/interest-rates/pages/textview.aspx?data=yield
- and using 253 (2020 is a leap year) number of trading days
- is calculated as (0.79 / 253) or :

```{r Optimization}
mu.free <- 0.79/253 ## input value of daily risk-free interest rate
mu.free
```




### Pessimism reigns

- A risk measure $\Large \rho$ is pessistic if, for some probability measure $\Large \phi$ on $\Large [0,1]$, 
$$
\Large
\rho(L) = \int_0^1 \rho_{u}(L) \phi(u) du.
$$

- For expected shortfall, $\Large \phi(u) = (1-\alpha)^{-1}I_{(u\geq\alpha)}$: equal weight is placed on all quantiles beyond the $\alpha$-quantile.
- Suppose we have a loss portfolio with position weights $\Large \pi$ and losses $X$ so that total loss is $\Large L = X\,'\pi$ with mean loss $\Large \mu(L)$. Let's choose loss weights to minimize
$$
\Large
min_{\pi}\,[\rho_{\alpha}(L) - \lambda \mu(L)] \,\, s.t.\, \mu(L)=\mu_0, \,\, 1^T\pi = 1
$$

where the weights add up to 1 and we try to achieve a minimum return $\Large \mu_0$.

- Taking this formulation to a sample version for $\Large n$ observations of losses, we get
$$
\Large
min_{\beta, \xi}\sum_{k=1}^m \, \sum_{i=1}^n \, \nu_k \rho_{\alpha}(X_{i1}-\sum_{j=2}^p (x_{i1}-x_{ij}\beta_{j})-\xi_k))
$$

$$
\Large
s.t.\, \bar{X}\,'\pi(\beta) = \mu_0
$$

- In this approach, there are $\Large m$ weights $\Large \nu$ that pull together $\Large m$ different sets of portfolio weightings. The $\Large \xi$ terms represent $\Large m$ different intercepts, one for each $\Large \nu_k$ weight. 
- There are $\Large p$ assets or loss categories here. We use the first asset, $\Large i = 1$ as the "numerarire" or benchmark asset. We measure returns on assets 2 to $\Large p$ relative to the first asset. The weights for assets 2 to $\Large p$ are the regression coeffients $\Large \beta$. the weight for the first asset uses the adding up constraint so that

$$
\Large
\pi_1 = 1 - \sum_{j=2}^p \pi_j
$$


The corresponding Markowitz (1952) approach is
$$
\Large
min_{\beta, \xi} \, \sum_{i=1}^n (X_{i1}-\sum_{j=2}^p (x_{i1}-x_{ij})\beta_{j}-\xi))^2
$$
subject to the constraint

$$
\Large
s.t.\, \bar{X}\,'\pi = \mu_0
$$

- We model distortions using weighted quantiles.
- The Choquet criterion ends up using a weighted average of quantile allocations across assessed probabilities to express preferences.
- Mimimize a weighted sum of quantile regression objective functions using the specified $\alpha$ quantiles. 
- The model permits distinct intercept parameters at each of the specified taus, but the slope parameters are constrained to be the same for all $\Large \alpha$s. 
- This estimator was originally suggested to the Roger Koenker by Bob Hogg in one of his famous blue book notes of 1979. 
- The algorithm used to solve the resulting linear programming problems is either the Frisch Newton algorithm described in Portnoy and Koenker (1997), or the closely related algorithm described in Koenker and Ng(2002) that handles linear inequality constraints. 
- Linear inequality constraints can be imposed.

```{r Quantiles, mysize=TRUE, size='\\footnotesize', echo = TRUE}
library(quantreg)
x <- data.r/100
n <- nrow(x)
p <- ncol(x)
alpha <-  c(0.1, 0.3) # quantiles
w <-  c(0.3, 0.7) # distortion weights
lambda <- 100 # Lagrange multiplier for adding up constraint
m <- length(alpha)
# error handling: if (length(w) != m) stop("length of w doesn't match length of alpha")
xbar <- apply(x, 2, mean)
mu.0 <-  mean(xbar)
y <- x[, 1] #set numeraire
r <- c(lambda * (xbar[1] - mu.0), -lambda * (xbar[1] - mu.0))
X <- x[, 1] - x[, -1]
R <- rbind(lambda * (xbar[1] - xbar[-1]), -lambda * (xbar[1] - xbar[-1]))
R <- cbind(matrix(0, nrow(R), m), R)
f <- rq.fit.hogg(X, y, taus = alpha, weights = w, R = R, r = r)
fit <- f$coefficients
# transform regression coeff to portfolio weights
pihat <- c(1 - sum(fit[-(1:m)]), fit[-(1:m)]) 
x <- as.matrix(x)
yhat <- x %*% pihat # predicted 
etahat <- quantile(yhat, alpha)
muhat <- mean(yhat)
qrisk <- 0
for (i in 1:length(alpha))
{
  qrisk <- qrisk + w[i] * sum(yhat[yhat < etahat[i]])/(n * alpha[i])
}
qrisk
pihat
```

Trying a different distortion

```{r Returns, mysize=TRUE, size='\\footnotesize', echo = TRUE}
library(quantreg)
#library(dplyr) # use data.df now
alpha <- 0.95
u <- quantile(data.df$returns.Corn, alpha )
x <- data.df.nd[data.df.nd$returns.Corn < u, 2:4]/100
n <- nrow(x)
p <- ncol(x)
alpha <-  c(0.01, 0.1) # quantiles at lower (negative) tail
w <-  c(0.95, 0.05) # distortion weights
lambda <- 100 # Lagrange multiplier for adding up constraint
m <- length(alpha) #alpha and w length must be the same
xbar <- apply(x, 2, mean)
mu.0 <-  mean(xbar)
y <- x[, 1] #set numeraire
r <- c(lambda * (xbar[1] - mu.0), -lambda * (xbar[1] - mu.0))
X <- x[, 1] - x[, -1] # set up design matrix of adjusted all but numeraire returns
R <- rbind(lambda * (xbar[1] - xbar[-1]), -lambda * (xbar[1] - xbar[-1])) # constraints
R <- cbind(matrix(0, nrow(R), m), R) #augmented constraints
f <- rq.fit.hogg(X, y, taus = alpha, weights = w, R = R, r = r) #Bob Hogg estimator
fit <- f$coefficients
# transform regression coeff to portfolio weights
pihat <- c(1 - sum(fit[-(1:m)]), fit[-(1:m)]) 
x <- as.matrix(x)
yhat <- x %*% pihat # predicted 
(etahat <- quantile(yhat, alpha))
(muhat <- mean(yhat))
qrisk <- 0
for (i in 1:length(alpha))
{
  qrisk <- qrisk + w[i] * sum(yhat[yhat < etahat[i]])/(n * alpha[i])
}
qrisk
pihat
```


### Extreme frontier finance

```{r Extreme Frontier}
mu.0 <- xbar
mu.P <- seq(-.0005, 0.0015, length = 100) ## set of 300 possible target portfolio returns
qrisk.P <-  mu.P ## set up storage for quantile risks of portfolio returns
weights <-  matrix(0, nrow=300, ncol = ncol(data.r)) ## storage for portfolio weights
colnames(weights) <- names(data.r)
for (i in 1:length(mu.P))
{
  mu.0 <-  mu.P[i]  ## target returns
  result <- qrisk(x, mu = mu.0)
  qrisk.P[i] <- -result$qrisk # convert to loss risk already weighted across alphas
  weights[i,] <-  result$pihat
}
qrisk.mu.df <- data.frame(qrisk.P = qrisk.P, mu.P = mu.P )
mu.P <- qrisk.mu.df$mu.P
##mu.free <-  0.00011 ## input value of risk-free interest rate
sharpe <- ( mu.P-mu.free)/qrisk.P ## compute Sharpe's ratios
ind <-  (sharpe == max(sharpe)) ## Find maximum Sharpe's ratio
ind2 <-  (qrisk.P == min(qrisk.P)) ## find the minimum variance portfolio
ind3 <-  (mu.P > mu.P[ind2]) ## find the efficient frontier (blue)
col.P <- ifelse(mu.P > mu.P[ind2], "blue", "grey")
weights.extr <- weights[ind,] # for use in calculating tengency risk measures
qrisk.mu.df$col.P <- col.P
eff_slope <- ((mu.P[ind]-mu.free) / qrisk.P[ind])
renderPlotly({
  p <- ggplot(qrisk.mu.df, aes(x = qrisk.P, y = mu.P, group = 1))
  p <- p + geom_line(aes(colour= col.P, group = col.P))
  p <- p + scale_colour_identity()  
  p <- p + geom_point(aes(x = 0, y = mu.free), colour = "red")
  options(digits=3)
  p <- p + geom_abline(intercept = mu.free, 
                       slope = eff_slope,
                       colour = "red")
  p <- p + geom_point(aes(x = qrisk.P[ind], y = mu.P[ind])) 
  p <- p + geom_point(aes(x = qrisk.P[ind2], y = mu.P[ind2])) 
  p	### ggplotly(p)
})
```

### Extreme portfolio risk measures

```{r Extreme Portfolio}
# price.last <- as.numeric(tail(data[, -1], n=1))
# Specify the positions
position.rf <- weights.extr #c(1/3, 1/3, 1/3)
# And compute the position weights
w <- position.rf * price.last
# Fan these  the length and breadth of the risk factor series
weights.rf <- matrix(w, nrow=nrow(data.r), ncol=ncol(data.r), byrow=TRUE)
#head(rowSums((exp(data.r/100)-1)*weights.rf), n=3)
## We need to compute exp(x) - 1 for very small x: expm1 accomplishes this
#head(rowSums((exp(data.r/100)-1)*weights.rf), n=4)
loss.rf <- -rowSums(expm1(data.r/100) * weights.rf)
alpha.tolerance <- 0.90
u <- quantile(loss.rf, alpha.tolerance, names = FALSE)
fit.extr <- fit.GPD(loss.rf, threshold = u)
renderPlot({
  showRM(fit.extr, alpha = alpha.tolerance, RM = "ES", method = "BFGS")
})
```

### Portfolio Analytics: the Markowitz model: default

```{r Portfolio Analytics}
library(quadprog)

R <- returns[,1:3]/100
quantile_R <- quantile(R[,1], 0.95)
#R <- subset(R, Corn > quantile_R, select = Corn:Wheat)
names.R <- colnames(R)
mean.R <-  apply(R,2,mean)
cov.R <-  cov(R)
sd.R <-  sqrt(diag(cov.R)) ## remember these are in daily percentages
#library(quadprog)
Amat <-  cbind(rep(1,3),mean.R)  ## set the equality constraints matrix
mu.P <- seq(min(mean.R), max(mean.R), length = 300)  ## set of 300 possible target portfolio returns
#mu.P <- seq(0.5*quantile_R, max(R), length = 100)  ## set of 300 possible target portfolio returns
sigma.P <-  mu.P ## set up storage for std dev's of portfolio returns
weights <-  matrix(0, nrow=300, ncol = ncol(R)) ## storage for portfolio weights
colnames(weights) <- names.R
for (i in 1:length(mu.P))
{
  bvec <- c(1,mu.P[i])  ## constraint vector
  result <- solve.QP(Dmat=2*cov.R,dvec=rep(0,3),Amat=Amat,bvec=bvec,meq=2)
  sigma.P[i] <- sqrt(result$value)
  weights[i,] <- result$solution
}
sigma.mu.df <- data.frame(sigma.P = sigma.P, mu.P = mu.P )
##mu.free <- .00011 ## input value of daily risk-free interest rate
sharpe <- ( mu.P-mu.free)/sigma.P ## compute Sharpe's ratios
ind <-  (sharpe == max(sharpe)) ## Find maximum Sharpe's ratio
ind2 <-  (sigma.P == min(sigma.P)) ## find the minimum variance portfolio
ind3 <-  (mu.P > mu.P[ind2]) ## finally the efficient frontier
col.P <- ifelse(mu.P > mu.P[ind2], "blue", "grey")
sigma.mu.df$col.P <- col.P
mark_default_slop <- ((mu.P[ind]-mu.free)/sigma.P[ind])
(mark_default_slop)
renderPlotly({
  p <- ggplot(sigma.mu.df, aes(x = sigma.P, y = mu.P, group = 1)) +
    geom_line(aes(colour=col.P, group = col.P)) +
    scale_colour_identity() # + xlim(0, max(sd.R*1.1))  + ylim(0, max(mean.R)*1.1) + 
  p <- p + geom_point(aes(x = 0, y = mu.free), colour = "red")
  options(digits=4)
  p <- p + geom_abline(intercept = mu.free, slope = mark_default_slop, colour = "red")
  p <- p + geom_point(aes(x = sigma.P[ind], y = mu.P[ind], pch="*")) 
  p <- p + geom_point(aes(x = sigma.P[ind2], y = mu.P[ind2], pch="-")) ## show min var portfolio
  p <- p + annotate("text", x = sd.R[1], y = 0.0001+mean.R[1], label = names.R[1]) +
    annotate("text", x = sd.R[2], y = 0.00025+mean.R[2], label = names.R[2]) +
    annotate("text", x = sd.R[3], y = -0.0001+mean.R[3], label = names.R[3])
  p	### ggplotly(p)
})
```

### Portfolio Analytics: the Markowitz model: no short

```{r Markowitz}
library(quadprog)

R <- returns[,1:3]/100
quantile_R <- quantile(R[,1], 0.95)
#R <- subset(R, Corn > quantile_R, select = Corn:Wheat)
names.R <- colnames(R)
mean.R <-  apply(R,2,mean)
cov.R <-  cov(R)
sd.R <-  sqrt(diag(cov.R)) ## remember these are in daily percentages
#library(quadprog)
Amat <-  cbind(rep(1,3),mean.R,diag(1,3))  ## set the equality constraints matrix
mu.P <- seq(min(mean.R), max(mean.R), length = 300)  ## set of 300 possible target portfolio returns
#mu.P <- seq(0.5*quantile_R, max(R), length = 100)  ## set of 300 possible target portfolio returns
sigma.P <-  mu.P ## set up storage for std dev's of portfolio returns
weights.x <-  matrix(0, nrow=300, ncol = ncol(R)) ## storage for portfolio weights.x
colnames(weights.x) <- names.R
for (i in 1:length(mu.P))
{
  bvec <- c(1,mu.P[i],rep(0,3)) ## no short sales
  result <- solve.QP(Dmat=2*cov.R,dvec=rep(0,3),Amat=Amat,bvec=bvec,meq=2)
  sigma.P[i] <- sqrt(result$value)
  weights.x[i,] <- result$solution
}
sigma.mu.df <- data.frame(sigma.P = sigma.P, mu.P = mu.P )
##mu.free <- .00011 ## input value of daily risk-free interest rate
sharpe <- ( mu.P-mu.free)/sigma.P ## compute Sharpe's ratios
inx <-  (sharpe == max(sharpe)) ## Find maximum Sharpe's ratio
inx2 <-  (sigma.P == min(sigma.P)) ## find the minimum variance portfolio
indx3 <-  (mu.P > mu.P[inx2]) ## finally the efficient frontier
col.P <- ifelse(mu.P > mu.P[inx2], "blue", "grey")
sigma.mu.df$col.P <- col.P
mark_no_short_slope <- ((mu.P[inx]-mu.free)/sigma.P[inx])
(mark_no_short_slope)
renderPlotly({
  p <- ggplot(sigma.mu.df, aes(x = sigma.P, y = mu.P, group = 1)) +
    geom_line(aes(colour=col.P, group = col.P)) +
    scale_colour_identity() # + xlim(0, max(sd.R*1.1))  + ylim(0, max(mean.R)*1.1) + 
  p <- p + geom_point(aes(x = 0, y = mu.free), colour = "red")
  options(digits=4)
  p <- p + geom_abline(intercept = mu.free, slope = mark_no_short_slope, colour = "red")
  p <- p + geom_point(aes(x = sigma.P[inx], y = mu.P[inx], pch="+")) 
  p <- p + geom_point(aes(x = sigma.P[inx2], y = mu.P[inx2], pch="-")) ## show min var portfolio
  p <- p + annotate("text", x = sd.R[1], y = 0.0001+mean.R[1], label = names.R[1]) +
    annotate("text", x = sd.R[2], y = 0.00025+mean.R[2], label = names.R[2]) +
    annotate("text", x = sd.R[3], y = -0.0001+mean.R[3], label = names.R[3])
  p	### ggplotly(p)
})
```



Conclusion
=======================================================================

Column 
-------------------------------------------------------------------------

### Skills and Tools

1. Packages: ggplot, scales, quadprog, quantreg, shiny, flexdashboard, qrmdata, xts, matrixStats, zoo, QRM, plotly, and psych

Skills used for data exploration and analytics are the following -

1. head() to return the first part of a vector, matrix, table, data frame or function.
2. tail() to return the last part of a vector, matrix, table, data frame or function.
3. summary() to produce result summaries of the results of various model fitting functions.
4. format() to format an R object for pretty printing.
5. diff() to return suitably lagged and iterated differences.
6. ifelse() to return a value with the same shape as test which is filled with elements selected from either yes or no depending on whether the element of test is TRUE or FALSE.
7. function() to create and store a function for data analysis.
8. round() to round the values in its first argument to the specified number of decimal places (default 0).
9. mean() is a generic function for the (trimmed) arithmetic mean.
10. ts() to create time-series objects.
11. rollapply() to apply a function to rolling margins of an array.
12. merge() to merge two data frames by common columns or row names, or do other versions of database join operations.
13. seq() to generate regular sequences.
14. rq() to perform a quantile regression on a design matrix, x, of explanatory variables and a vector, y, of responses.
15. lm() can be used to carry out regression, single stratum analysis of variance and analysis of covariance.
16. Plot() to make a map of the values of a Raster* object, or make a scatterplot of their values.
17. split() to divide the data in the vector x into the groups defined by f. 
18. lapply() to return a list of the same length as X, each element of which is the result of applying FUN to the corresponding element of X.
19. image_animate() manipulate or combine multiple frames of an image. 
20. ncol() returns the number of columns.
21. cor() computes the variance of x and the covariance or correlation of x and y.
22. apply.monthly() applies a specified function to each distinct period in a given time series object.
22. quantile() produces sample quantiles corresponding to the given probabilities.
23. apply.monthly() applies a specified function to each distinct period in a given time series object.
24. max() returns the maximum of all values in a vector by passing codemax as fn argument to univar function.

Skills used in packaging and express of data in terms of graphs and tables are the following -

1. ggplot()/ggplot2() to generate the graph based on the vectors.
2. kable() to create a data table.
3. table() returns a contingency table.
4. xtable to create LaTeX formatted and rendered table.
5. ggplotly() to convert a ggplot2::ggplot() object to a plotly object.
6. autoplot.zoo() takes a zoo object and converts it into a data frame (intended for ggplot2)
7. ggtitle() to format chart titles.
8. ylim() to to set the limits of the y axis.
9. coredata() to extract the core data contained in a (more complex) object and replacing it.
10. acf() to compute (and by default plots) estimates of the autocovariance or autocorrelation function.
11. pacf() is the function used for the partial autocorrelations.


Column 
-------------------------------------------------------------------------

### Portfolio Weights

For the working capital accounts:

No constraints:

```{r echo = FALSE}
library(scales)
weights[ind,]
name <- colnames(weights)
posn <- ifelse((weights[ind,]<0), "go short (sell)", "go long, (buy)")
value <- percent(abs(weights[ind,]))
weights[ind,]*250000000
```

* <TODO> - Update numbers
* 250,000,000 denominated in euros
* Buy 250,236,183 in Corn
* Sell 1,577,898 in Soybean
* Buy 1,341,715 in Wheat


With constraints:

Constraint: Minimum Variance Portfolio
```{r echo = FALSE}
library(scales)
weights[ind2,]
name <- colnames(weights)
posn <- ifelse((weights[ind2,]<0), "go short (sell)", "go long, (buy)")
value <- percent(abs(weights[ind2,]))
weights[ind2,]*250000000
```

* <TODO> - Update numbers
* 250,000,000 denominated in euros
* Buy 21,438,648 in Corn
* Buy 113,727,083 in Soybean
* Buy 114,834,269 in Wheat

Constraint: No Short Portfolio

```{r echo = FALSE}
library(scales)
weights.x[inx,]
name <- colnames(weights.x)
posn <- ifelse((weights.x[inx,]<0), "go short (sell)", "go long, (buy)")
value <- percent(abs(weights.x[inx,]))
weights.x[inx,]*250000000
```

* <TODO> - Update numbers
* 250,000,000 denominated in euros
* Buy 250,000,000 in Corn

The weights for the Markowitz tangency [default] portfolio ("*") are

```{r echo = FALSE}
library(scales)
weights[ind,]
name <- colnames(weights)
posn <- ifelse((weights[ind,]<0), "go short (sell)", "go long, (buy)")
value <- percent(abs(weights[ind,]))
```

The weights for the Markowitz tangency [min var]  portfolio  ("-") are

```{r echo = FALSE}
library(scales)
weights[ind2,]
name <- colnames(weights)
posn <- ifelse((weights[ind2,]<0), "go short (sell)", "go long, (buy)")
value <- percent(abs(weights[ind2,]))
```

The weights for the Markowitz tangency [no short] portfolio ("+") are

```{r echo = FALSE}
library(scales)
weights.x[inx,]
name <- colnames(weights.x)
posn <- ifelse((weights.x[inx,]<0), "go short (sell)", "go long, (buy)")
value <- percent(abs(weights.x[inx,]))
```


Column 
-------------------------------------------------------------------------

### Business Remarks

- <TODO> Update all bullets below, these numbers and information is incorrect

- The working capital account of \$250 million euro should be allocated as follows: buy \$xxx million in Corn, \$yyy million in Soybean, \$zzz million in Wheat.

- There is a strong correlation (0.89) between Ingrediant-A (Corn) and Ingrediant-B (Soybean) due to shared applications.

- Creation of cuproCorn alloy which is used for desalinisation due its high resistant to corrosion, minting, armaments, marine engineering, electrical applications, and many others, e.g. the repair of fan blades found in geothermal power plants. 
    
- Another example would be purchasing Soybean from Codelco in Santiago, Chile and tramping it to General Electric, Schnectady, NY as part of GE's purchasing and procurement wing of their supply chain.  

- Wheat is an ingrediant which is used in a plethora of industries and markets.  It's relatively stable price is testament to this.  Moving Wheat from Brasil to West Coast USA for aircraft supply chains is a safe, long term freight line that ship owner's can use to mitigate risk.


### Recommendations

Why did you choose those weights (which model)?

- <TODO>
  
How does your decision impact the business?

- <TODO>

What are your recommendations?

- <TODO>




References
=======================================================================

### REFERENCES

**References**

Artzner, P., F. Delbaen, J.-M. Eber, and D. Heath (1999), Coherent measures of
risk, Mathematical Finance, 9:203???228.

Bassett, G., R. Koenker, G. Kordas (2004), Pessimistic Portfolio Allocation and Choquet Expected Utility, Journal of Financial Econometrics, 2, 477-492.

Choquet, G. (1953), Theory of Capacities, Annales de l'Institut Fourier 5, pages 131-295.

Cox, D. (1962), Comment on L. J. Savage's Lecture "Subjective Probability and Statistical Practice", in The Foundations of Statistical Inference (ed. by G. Barnard and D. Cox), London: Methuen.

Koenker, R. and Ng, P (2003), Inequality Constrained Quantile Regression, preprint.

Koenker, Roger (2005), Quantile Regression (Econometric Society Monographs), Cambridge University Press.

Koenker, R. (1984), A note on L-estimates for linear models, Stat. and Prob Letters, 2, 323-5.

Markowitz, Harry (1952), Portfolio Selection, The Journal of Finance, Vol. 7, No. 1, pp. 77-91.

McNeill, Alexander J., Rudiger Frey, and Paul Embrechts (2015), Quantitative Risk Management: Concepts, Techniques and Tools. Revised Edition. Princeton: Princeton University Press.

Polbennikov, S. and B. Melenberg (2005), Mean-Coherent Risk and Mean-Variance Approaches in Portfolio Selection: an Empirical Comparison, Discussion Papers 2005 ??? 013, Tilburg University.

Portnoy, S. and Koenker, R. (1997), The Gaussian Hare and the Laplacean Tortoise: Computability of Squared-error vs Absolute Error Estimators, (with discussion). Statistical Science, (1997) 12, 279-300.

Rockafellar, R. T. and S. Uryasev (2000), Optimization of conditional value-at-risk.
The Journal of Risk, 2:21???41.

Ruppert, David and David S. Matteson (2014), Statistics and Data Analysis for Financial Engineering with R Examples, Second Edition. New York: Springer. 

Schmeidler, D. (1989), Subjective Probability and Expected Utility Without Additivity, Econometrica, 57, 571-587.

Sharpe, William F. (1966), Mutual Fund Performance, Journal of Business, January 1966, 119-138.

Tasche, D. (2000), Conditional expectation as a quantile derivative, Preprint, TU-Munchen. (Available from arXiv math/0104190.)

Tversky, A and Kahneman, D. (1992), Advances in Prospect Theory: Cumulative representation of uncertainty. Journal of Risk and Uncertainty, 5(4): 297-323.

von Nuemann, J. and Morgenstern, O. (1944), Theory of games and economic behaviour, Princeton University Press. 

Zou, Hui and and Ming Yuan (2008), Composite quantile regression and the Oracle model selection theory, Annals of Statistics, 36, 1108-11120.


